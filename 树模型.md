---
typora-root-url: ./
---

# 								树模型

#### 决策树的训练与预测：

**训练**：

决策树训练时，会根据训练集选择每个特征(+取值)来划分数据集，直到数据集不可分或规模太小/树深度达到阈值/训练误差小于阈值（回归）/特征的信息增益小于阈值/数据集基尼指数小于阈值（够纯）。这时会停止分割，将落入该节点的多数训练样本的类别作为该节点的最终预测类别（分类）/  将落入该节点的所有训练样本的均值，作为该节点的最终预测值(回归。训练误差最小)。

训练结束后，会保存树的结构以及每个节点的信息，包括内部节点的特征(以及取值）和叶子节点的预测类别（分类/预测值(回归)。此外，训练过程中在每一步计算得到的特征重要性(分类：信息增益/信息增益比/基尼指数，回归：预测误差)也会保存下来

**预测**： 将样本特征与训练得到的特征（和取值）进行比较，递归向下，落到某叶节点，得到最终的预测分类/预测值

#### 特征选择与特征处理：

**ID3：**只能处理离散特征。每次都选择当前信息增益$g(D,A)$最大的特征$A$作为分割节点。该特征的信息增益是原始数据集上的信息熵(相对类别)与被特征$A$分割后的子数据集上的条件熵$H(D|A)$的差值:
$$
g(D,A)=H(D)-H(D|A)
$$
其中条件熵$H(D|A)$ 是特征$A$的每个取值$A_i=c$对应的子数据集$D_i$的加权熵($p_i$是特征$A$取值为$c$的样本占比)：
$$
H(D|A)=p_i\sum_{i=1}^KH(Di)
$$
信息增益本质上是因为知道了特征A而减少了的不确定性。该值越大，说明特征A带来的信息量越多。选择特征A能够最大的减少剩余信息的不确定性，使得子数据集整体的熵减小，划分后子数据集比较纯，分类较好。本质上是选择让剩余数据集信息量(熵)最小的特征。

由于ID3只能处理离散特征，所以每个节点对应的子数都是多分叉树，对应该特征的所有取值。因此在用完该特征后，下一次进行分割时，子数据集应丢弃用过的特征。

**C4.5:**   ID3倾向于首先选择取值比较多，但样本数少的特征（高基数特征，high-cardinality），前提是某个特征的每个取值下的样本数非常少（这时候条件熵$H(D|A)$会较小。否则没有明显的偏向性）。因为这样会迅速把样本空间划分的过小，增加过拟合的风险。比如ID类特征（IP，地址，门牌，姓名）。用这类特征划分子数据集后，每个子数据集只有少量样本，划分后每个子数据集熵都是0，信息增益大，但分割后难以泛化。为了避免首先选择这类特征，需要对这类特征进行惩罚。C4.5用信息增益比代替信息增益，用特征本身的熵做惩罚：
$$
g_R(D.A)=\frac{g(D,A)}{H_A(D)}
$$
​        其中$H_A(D)$是关于特征A本身的熵：$-p(Ai=c)log(p(Ai=c))$。熵越大，说明特征本身不确定性较大，取值可能较多且等概率，会把样本空间划分的较小，不太好泛化 。以ID为例，有n种取值，每种取值等概率，熵就会比较大，用特征的熵做乘法，就可以尽量优先选那些取值较少的特征:
$$
H_A(D)=-\sum_{i=1}^Kp(A_i)log(p(A_i))=-\sum_{i=1}^n\frac{1}{n}log\frac{1}{n}=-log\frac{1}{n}=logn
$$
​       但是信息增益率本身引入了特征的分布属性，偏向于分布不平衡（特征熵较小）的特征。而信息增益本身更能够反应划分的好坏。因此为保证划分质量，首先会先计算所有特征的平均信息增益作为阈值。只有大于该信息增益的特征，才会进一步用信息增益率做选择。

​        对于离散特征，C4.5同ID3, 也是选择该特征所有的可能取值进行多子树划分，特征用完之后就丢弃掉。只是多加了特征熵的惩罚。

​         对连续性特征，C4.5会对将该样本的取值离散化，将该特征在有限训练样本中的所有取值进行排序，取所有中间值作为分割点进行二分分割。因为都是二分分割，在比较该特征的所有取值时，若选择信息增益率作为指标，在分割点差不多的情况下，会惩罚等分分割点（特征熵比较大）。在能够较好划分样本的情况下，可能因为惩罚而选择不平衡的分割点。因此在选择连续特征的最优分割点时，只用分割点处的信息增益做指标，尽量将两部分数据分开。

​       选定每个连续特征的最优分割点后，需计算信息增益率，和离散特征进行比较（相当于都加上对取值的惩罚）。而对于每个连续型特征，在每一步选择完最优分割点后，若还有其他分割点，之后的数据集仍可以用这些分割点进行划分。

​    [参考： C4.5 连续特征的处理](https://www.cnblogs.com/wf-ml/p/10685499.html)

​    [参考：C4.5 连续特征的处理2](https://www.cnblogs.com/zhangchaoyang/articles/2842490.html)

**CART:** 二叉分裂。在特征选择时，遍历每个特征的每个取值，在所有组合中，选择最优的组合$(A,a_i)$对数据集进行二分切割，每一步不仅保存特征本身，也保存该特征的取值，以对新样本进行预测。离散特征的每个取值都是一个切分点，根据特征$A$取值是否是$ai$对数据集二分。连续特征的二分切割同C4.5，用该特征的所有取值的中间值做切分，同样是所有特征的所有取值同时比较，选择当前的最优切分。

​          对于分类问题，CART采用基尼指数来衡量切分的好坏：
$$
G_{ini}(D)=\sum_{i=1}^Kp_i(1-p_i)
$$
 其中$p_i$为该类的占比。类似于熵，基尼指数越小，说明数据纯度越高，切分得越好。因此选择切分后，加权基尼指数最小的特征$A$和对应取值$ai$作为最优分割点：
$$
G_{ini}(D,A=a_i)=\frac{D1}{D}G_{ini}(D1)+\frac{D2}{D}G_{ini}(D2)
$$
​        CART也可以应用于回归问题，选择训练均方误差最小的特征（取值）作为最佳切分点。对于每个特征$j$的每个取值$s$划分的两个子节点，在切分点固定的情况下，每个子节点都对应一个固定的预测值$c_i$。落入该子节点的所有训练样本都会被预测为$c_i$。如果划分好的的两个样本空间R1，R2对应的两个预测值分别是$c_1$,$c_2$, 划分后总的训练误差是每部分训练误差的和：
$$
L(j,s)= \sum_{\vec{x_i}\in{R_1}}{(y_i-c_1)^2} +  \sum_{\vec{x_i}\in{R_2}}{(y_i-c_2)^2}
$$
而对于每个固定的切分，能最小化训练误差的预测值是是落入该节点的样本取值的均值：
$$
\hat{c}_1=avg(y_i|\vec{x_i}\in{R_1})
$$
因此特征选择可以看作是选能够使划分后的两部分数据均方误差之和最小的切分组合$(j,s)$：
$$
L(j,s)= \sum_{\vec{x_i}\in{R_1}}{(y_i-m_1)^2} +  \sum_{\vec{x_i}\in{R_2}}{(y_i-m_2)^2}
$$
选定当前特征和对应取值后，该特征如果有其他切分点，在之后的切分中仍可以用来切分。当某次切分后总的训练误差小于指定阈值，或者满足其他停止条件时，停止切分，把当前节点作为叶子节点，对应的预测值为落入该节点训练样本的均值c。之后将该值作为对落入本节点的样本的回归预测值$y_{pred}$。

相比前两种决策树，CART不需要计算log，计算复杂度也降低了。

#### 决策树剪枝：

**ID3/C4.5**:用带有叶节点数目做惩罚项的损失函数来定义一个树的好坏，希望剪枝后的树能够在有效区分样本的基础上，适当降低复杂度：
$$
L(T)=C(T)+\alpha|T|
$$
其中$C(T)$代表树T对训练样本本身的拟合程度，每一项是落到每个叶节点t的所有样本数目$N_t$与这些样本的熵$Ht$的乘积。模型对样本划分的越好，落到越纯的叶节点的样本数目越多，C(T)越小，模型拟合程度越好:
$$
C(T)=\sum_{t}N_tH_t
$$
而$\alpha|T|$是惩罚项，防止模型过拟合，希望选较简单的模型。

在生成一颗完整的树后，对每个叶子节点都计算剪枝前后的$L(T)$,$L(T-1)$,如果损失函数减小就剪枝，父节点变成叶节点。递归所有的叶节点，直到损失函数最小，得到最优的剪枝后的决策树

**CART：**不是直接根据损失函数递归向上剪枝，而是对当前已经生成的树$T$=$T_0$，每次选一个误差增加率最小的内部节点t进行剪枝，得到少了部分节点的树$T_1$。之后继续在$T_1$的基础上进行修建。得到一个从原始的树$T_0$开始，到$T_n$（只剩3个节点，一根两叶子）的子树序列:$\{T_0,T_1,...T_n\}$，之后在其中选择误差最小的树作为树$T$的最优剪枝树。

由于每次剪去一个内部节点，之前落到该子树的划分都不复存在，在训练集/验证集上的分类/预测误差一定会增加，但好处是叶子节点变少，树整体结构会变简单，降低过拟合的风险。因此定义**误差增加率**为：剪去该节点后的误差增加与叶子节点减少的数目的比例：
$$
\alpha=\frac{L(t)-L(T_t)}{|T_t|-1}
$$
其中$L(t)$是内部节点$t$作为叶子节点时的误差（比如落到该节点的所有样本的基尼指数/预测误差），$L(T_t)$是剪枝前该子数的基尼指数/预测误差。剪枝后，所有原本落到t的子树的样本都落到节点$t$，导致样本不够纯，基尼指数/预测误差会增加。$|T_t|-1$是因为对剪枝减少的叶子节点数目。其中$|T_t|$是以$t$为根的子数，叶节点的数目。

具体来说：

1. 首先采用K者交叉验证，得到K个完全生长的决策树
2. 令每个决策树$T$=$T_0$
3. 对树$T$的所有内部节点$t$计算误差增加率，选择剪去一个内部节点后，误差增长率最小的子树，加入该决策树的子树序列：$T$=$T_i$。其中误差采用训练误差。
4. 重复2，3，不断得到更小的树加入子树序列。直到最后的子树$T_n$只剩下2个叶子节点：$\{T_0,T_1,...T_n\}$
5. 从4中得到的子树序列中选择最优子树，作为决策树T的最佳剪枝树。(用第K折数据作选择)
6. K个决策树再在dev集上做验证，得到最终的最优剪枝树

#### 方差偏差：

模型的方差，偏差，都是泛化误差的一部分。可以看作对过拟合，欠拟合的一种定量分析。这里的模型指的是该模型代表的函数空间，如线性模型，高阶模型等，而非某数据集训练得到的具体模型。

偏差：用来刻画模型本身的学习能力，是不同采样数据集下下，不同模型预测的均值与样本真实值之间的差距。模型越复杂，越能够拟合预测值，偏差越小。如果模型总有较大偏差，说明模型学习能力不足，所代表的函数空间没法拟合数据空间。一般训练集上就能看到偏差，无法拟合训练集。

![微信截图_2](/树模型.assets/微信截图_2.png)

方差：是模型在用同一数据集的不同采样进行训练后，预测结果的稳定性。如果不同采样数据集下训练出来的模型，预测结果都差不多（方差较小），说明模型对输入样本的分布不敏感，能够较好的捕捉不同数据集中共性的规律，有较好的泛化能力。而复杂模型学的太细，容易过拟合，如果数据集稍有改变，拟合曲线就会发生较大变化，导致方差较大。如果线性模型，尽管单个模型偏差大，但对数据的扰动更加不敏感，方差小：

![微信截图_20210207201541](/树模型.assets/微信截图_20210207201541.png)

更直观的看，越复杂的模型方差越大，对输入值越敏感，越容易过拟合：

![微信截图_20210207204452](/树模型.assets/微信截图_20210207204452.png)

#### 集成学习

**Bagging**：综合多个不同的模型得到的结果进行预测。这样得到的结果往往好于独立的单个模型。因为独立实验次数越多，得到的结果越接近于数据的真实值（大数定理）。即使是稍微好于随机预测的弱模型（准确率51%），单个模型犯错的概率是49%，如果有多个这样的独立模型，综合犯错的概率会很低（集体智慧/3个臭皮匠）。因此可以通过对弱模型的组合，来得到强模型。

严格来说，如果每个弱分类器预测错误的概率是：
$$
P(f(x)!=y)=\epsilon
$$
N个弱分类器的预测结果是所有模型结果的综合：
$$
F(x)=sign(\sum_{i=1}^{N}f(x))
$$
只有超过一半的分类器预测错误，最终的模型才会预测错误。因此集成后，模型最终预测错误的概率为其中至少一半基模型预测错误的概率。根据Hoeffding不等式，模型的预测误差会随着基模型数目的增多而指数减小。理论上讲，无数个独立模型可以让最终的预测误差趋近于0：
$$
P(F(x)!=y)=\sum_{k=0}^{N/2}C_N^k \epsilon^k(1-\epsilon)^k \le e^{(-\frac{1}{2}N(2\epsilon-1)^2)}
$$

因此在项目的最后阶段，往往用不同的模型分别做预测，综合各个模型的结果，得到更好的效果（ensemble）。

另一方面，从方差偏差的角度讲，综合多个基模型的结果进行预测，整体的方差会降低为原来的$1/n$，最终得到的预测结就有小的方差，预测结果有较好的稳定性。

为了增加不同模型的独立性，一般选择不同的采样数据集/不同模型/不同特征进行训练。最好选择方差较大，对样本分布较敏感的模型作为基模型(如决策树，神经网络)，使得不同的采样下，模型会有较大不同。Bagging采用有放回抽样得到子数据集：如果原始训练集数目为$m$, Bagging会有放回的方式抽$n$次，得到大小为$n$的采样集($n$可以等于$m$)。其中有的样本被采样多次，有的样本没有被采到。这个过程重复T次，得到T个采样集，用来训练每个基模型。这些采样集中的模型分布不同，因此训练得到的基模型也相对独立。而对每个采样集而言，原始数据集中约有36.8%的样本未使用，因此可以用这些包外样本对该基模型进行评估，降低模型整体的泛化误差，不需要额外的验证集。

但bagging模型不会降低基模型的偏差，模型整体的偏差基本近似于基模型的偏差，所以一般采用偏差小的强模型做基模型。对于决策树，可以不剪枝，增大树的深度。但同时，bagging也应尽量避免选择对样本分布不敏感，方差较小的稳定模型做基模型（SVM，线性回归等）：基模型本身方差就小，不同采样得到的基分类器基本相同。集成就不能改善模型。本质上，集成是通过对基模型预测结果的平滑，让模型的结果变好。当基模型很稳定时，集成的效果有限。但如果基模型不稳定但准确时，集成能有效降低误差。

bagging有放回采样会使得模型独立性增加，样本分布有较大的不同。也可以考虑patching（在采样每个子数据集时无放回），但会使基模型独立性较低。

**Boost**：用不同模型逐渐对预测值进行逼近，每一步都聚焦于之前分错的样本，逐步减小偏差。由于各模型之间并不独立，boost不能降低方差。boost方法的方差近似于基模型的方差。因此可以选择方差小的弱模型（弱模型,如lr）作为基模型。

#### 为什么决策树对输入样本敏感？

决策树的训练结果非常依赖于训练样本的分布情况。训练集中样本分布的微小改变，会导致子数据集的条件熵$H(D|A)$/基尼指数$G_ini(D,A)$的值发生改变，使得最终选择的特征不同，树结构改变。因此决策树对样本的整体分布较为敏感。尤其在较深的情况下，有较高的方差，较低的偏差。

其他分类器如果对样本分布不敏感，不同数据下bagging得到的模型很相似，非独立模型，很难通过bagging的方式降低泛化误差。和一个模型没有太大差别，方差不会明显降低。

#### 随机森林

为了进一步增加不同基模型之间的独立性，相比于一般的Bagging方法，对于每个采样子集，随机森林从所有特征中随机选择k个特征作为该基模型的特征。根据该特征子集建立CART模型。

#### 特征重要性

一般是树选择该特征切分时，整体降低的基尼指数（纯度）。降低的纯度越多，说明该特征越重要。但这样得到的特征重要性会更偏向于取值多的特征（高基特征，如ID）。可以选择Permutation feature importance，对每个特征进行打乱，看验证集上模型的表现下降的程度：该特征打乱后性能下降越多，该特征越重要。

ADABOOST

#### GBDT

#### XGBoost







