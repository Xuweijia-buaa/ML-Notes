---
typora-root-url: ./
---

# 								树模型

#### 决策树的训练与预测：

**训练**：

决策树训练时，会根据训练集选择每个特征(+取值)来划分数据集，直到数据集不可分或规模太小/树深度达到阈值/训练误差小于阈值（回归）/特征的信息增益小于阈值/数据集基尼指数小于阈值（够纯）。这时会停止分割，将落入该节点的多数训练样本的类别作为该节点的最终预测类别（分类）/  将落入该节点的所有训练样本的均值，作为该节点的最终预测值(回归。训练误差最小)。

训练结束后，会保存树的结构以及每个节点的信息，包括内部节点的特征(以及取值）和叶子节点的预测类别（分类/预测值(回归)。此外，训练过程中在每一步计算得到的特征重要性(分类：信息增益/信息增益比/基尼指数，回归：预测误差)也会保存下来

**预测**： 将样本特征与训练得到的特征（和取值）进行比较，递归向下，落到某叶节点，得到最终的预测分类/预测值

#### 特征选择与特征处理：

**ID3：**只能处理离散特征。每次都选择当前信息增益$g(D,A)$最大的特征$A$作为分割节点。该特征的信息增益是原始数据集上的信息熵(相对类别)与被特征$A$分割后的子数据集上的条件熵$H(D|A)$的差值:
$$
g(D,A)=H(D)-H(D|A)
$$
其中条件熵$H(D|A)$ 是特征$A$的每个取值$A_i=c$对应的子数据集$D_i$的加权熵($p_i$是特征$A$取值为$c$的样本占比)：
$$
H(D|A)=p_i\sum_{i=1}^KH(Di)
$$
信息增益本质上是因为知道了特征A而减少了的不确定性。该值越大，说明特征A带来的信息量越多。选择特征A能够最大的减少剩余信息的不确定性，使得子数据集整体的熵减小，划分后子数据集比较纯，分类较好。本质上是选择让剩余数据集信息量(熵)最小的特征。

由于ID3只能处理离散特征，所以每个节点对应的子数都是多分叉树，对应该特征的所有取值。因此在用完该特征后，下一次进行分割时，子数据集应丢弃用过的特征。

**C4.5:**   ID3倾向于首先选择取值比较多，但样本数少的特征（高基数特征，high-cardinality），前提是某个特征的每个取值下的样本数非常少（这时候条件熵$H(D|A)$会较小。否则没有明显的偏向性）。因为这样会迅速把样本空间划分的过小，增加过拟合的风险。比如ID类特征（IP，地址，门牌，姓名）。用这类特征划分子数据集后，每个子数据集只有少量样本，划分后每个子数据集熵都是0，信息增益大，但分割后难以泛化。为了避免首先选择这类特征，需要对这类特征进行惩罚。C4.5用信息增益比代替信息增益，用特征本身的熵做惩罚：
$$
g_R(D.A)=\frac{g(D,A)}{H_A(D)}
$$
​        其中$H_A(D)$是关于特征A本身的熵：$-p(Ai=c)log(p(Ai=c))$。熵越大，说明特征本身不确定性较大，取值可能较多且等概率，会把样本空间划分的较小，不太好泛化 。以ID为例，有n种取值，每种取值等概率，熵就会比较大，用特征的熵做乘法，就可以尽量优先选那些取值较少的特征:
$$
H_A(D)=-\sum_{i=1}^Kp(A_i)log(p(A_i))=-\sum_{i=1}^n\frac{1}{n}log\frac{1}{n}=-log\frac{1}{n}=logn
$$
​       但是信息增益率本身引入了特征的分布属性，偏向于分布不平衡（特征熵较小）的特征。而信息增益本身更能够反应划分的好坏。因此为保证划分质量，首先会先计算所有特征的平均信息增益作为阈值。只有大于该信息增益的特征，才会进一步用信息增益率做选择。

​        对于离散特征，C4.5同ID3, 也是选择该特征所有的可能取值进行多子树划分，特征用完之后就丢弃掉。只是多加了特征熵的惩罚。

​         对连续性特征，C4.5会对将该样本的取值离散化，将该特征在有限训练样本中的所有取值进行排序，取所有中间值作为分割点进行二分分割。因为都是二分分割，在比较该特征的所有取值时，若选择信息增益率作为指标，在分割点差不多的情况下，会惩罚等分分割点（特征熵比较大）。在能够较好划分样本的情况下，可能因为惩罚而选择不平衡的分割点。因此在选择连续特征的最优分割点时，只用分割点处的信息增益做指标，尽量将两部分数据分开。

​       选定每个连续特征的最优分割点后，需计算信息增益率，和离散特征进行比较（相当于都加上对取值的惩罚）。而对于每个连续型特征，在每一步选择完最优分割点后，若还有其他分割点，之后的数据集仍可以用这些分割点进行划分。

​    [参考： C4.5 连续特征的处理](https://www.cnblogs.com/wf-ml/p/10685499.html)

​    [参考：C4.5 连续特征的处理2](https://www.cnblogs.com/zhangchaoyang/articles/2842490.html)

**CART:** 二叉分裂。在特征选择时，遍历每个特征的每个取值，在所有组合中，选择最优的组合$(A,a_i)$对数据集进行二分切割，每一步不仅保存特征本身，也保存该特征的取值，以对新样本进行预测。离散特征的每个取值都是一个切分点，根据特征$A$取值是否是$ai$对数据集二分。连续特征的二分切割同C4.5，用该特征的所有取值的中间值做切分，同样是所有特征的所有取值同时比较，选择当前的最优切分。

​          对于分类问题，CART采用基尼指数来衡量切分的好坏：
$$
G_{ini}(D)=\sum_{i=1}^Kp_i(1-p_i)
$$
 其中$p_i$为该类的占比。类似于熵，基尼指数越小，说明数据纯度越高，切分得越好。因此选择切分后，加权基尼指数最小的特征$A$和对应取值$ai$作为最优分割点：
$$
G_{ini}(D,A=a_i)=\frac{D1}{D}G_{ini}(D1)+\frac{D2}{D}G_{ini}(D2)
$$
​        CART也可以应用于回归问题，选择训练均方误差最小的特征（取值）作为最佳切分点。对于每个特征$j$的每个取值$s$划分的两个子节点，在切分点固定的情况下，每个子节点都对应一个固定的预测值$c_i$。落入该子节点的所有训练样本都会被预测为$c_i$。如果划分好的的两个样本空间R1，R2对应的两个预测值分别是$c_1$,$c_2$, 划分后总的训练误差是每部分训练误差的和：
$$
L(j,s)= \sum_{\vec{x_i}\in{R_1}}{(y_i-c_1)^2} +  \sum_{\vec{x_i}\in{R_2}}{(y_i-c_2)^2}
$$
而对于每个固定的切分，能最小化训练误差的预测值是是落入该节点的样本取值的均值：
$$
\hat{c}_1=avg(y_i|\vec{x_i}\in{R_1})
$$
因此特征选择可以看作是选能够使划分后的两部分数据均方误差之和最小的切分组合$(j,s)$：
$$
L(j,s)= \sum_{\vec{x_i}\in{R_1}}{(y_i-m_1)^2} +  \sum_{\vec{x_i}\in{R_2}}{(y_i-m_2)^2}
$$
选定当前特征和对应取值后，该特征如果有其他切分点，在之后的切分中仍可以用来切分。当某次切分后总的训练误差小于指定阈值，或者满足其他停止条件时，停止切分，把当前节点作为叶子节点，对应的预测值为落入该节点训练样本的均值c。之后将该值作为对落入本节点的样本的回归预测值$y_{pred}$。

相比前两种决策树，CART不需要计算log，计算复杂度也降低了。

#### 决策树剪枝：

**ID3/C4.5**:用带有叶节点数目做惩罚项的损失函数来定义一个树的好坏，希望剪枝后的树能够在有效区分样本的基础上，适当降低复杂度：
$$
L(T)=C(T)+\alpha|T|
$$
其中$C(T)$代表树T对训练样本本身的拟合程度，每一项是落到每个叶节点t的所有样本数目$N_t$与这些样本的熵$Ht$的乘积。模型对样本划分的越好，落到越纯的叶节点的样本数目越多，C(T)越小，模型拟合程度越好:
$$
C(T)=\sum_{t}N_tH_t
$$
而$\alpha|T|$是惩罚项，防止模型过拟合，希望选较简单的模型。

在生成一颗完整的树后，对每个叶子节点都计算剪枝前后的$L(T)$,$L(T-1)$,如果损失函数减小就剪枝，父节点变成叶节点。递归所有的叶节点，直到损失函数最小，得到最优的剪枝后的决策树

**CART：**不是直接根据损失函数递归向上剪枝，而是对当前已经生成的树$T$=$T_0$，每次选一个误差增加率最小的内部节点t进行剪枝，得到少了部分节点的树$T_1$。之后继续在$T_1$的基础上进行修建。得到一个从原始的树$T_0$开始，到$T_n$（只剩3个节点，一根两叶子）的子树序列:$\{T_0,T_1,...T_n\}$，之后在其中选择误差最小的树作为树$T$的最优剪枝树。

由于每次剪去一个内部节点，之前落到该子树的划分都不复存在，在训练集/验证集上的分类/预测误差一定会增加，但好处是叶子节点变少，树整体结构会变简单，降低过拟合的风险。因此定义**误差增加率**为：剪去该节点后的误差增加与叶子节点减少的数目的比例：
$$
\alpha=\frac{L(t)-L(T_t)}{|T_t|-1}
$$
其中$L(t)$是内部节点$t$作为叶子节点时的误差（比如落到该节点的所有样本的基尼指数/预测误差），$L(T_t)$是剪枝前该子数的基尼指数/预测误差。剪枝后，所有原本落到t的子树的样本都落到节点$t$，导致样本不够纯，基尼指数/预测误差会增加。$|T_t|-1$是因为对剪枝减少的叶子节点数目。其中$|T_t|$是以$t$为根的子数，叶节点的数目。

具体来说：

1. 首先采用K者交叉验证，得到K个完全生长的决策树
2. 令每个决策树$T$=$T_0$
3. 对树$T$的所有内部节点$t$计算误差增加率，选择剪去一个内部节点后，误差增长率最小的子树，加入该决策树的子树序列：$T$=$T_i$。其中误差采用训练误差。
4. 重复2，3，不断得到更小的树加入子树序列。直到最后的子树$T_n$只剩下2个叶子节点：$\{T_0,T_1,...T_n\}$
5. 从4中得到的子树序列中选择最优子树，作为决策树T的最佳剪枝树。(用第K折数据作选择)
6. K个决策树再在dev集上做验证，得到最终的最优剪枝树

#### 方差偏差：

模型的方差，偏差，都是泛化误差的一部分。可以看作对过拟合，欠拟合的一种定量分析。这里的模型指的是该模型代表的函数空间，如线性模型，高阶模型等，而非某数据集训练得到的具体模型。

偏差：用来刻画模型本身的学习能力，是不同采样数据集下下，不同模型预测的均值与样本真实值之间的差距。模型越复杂，越能够拟合预测值，偏差越小。如果模型总有较大偏差，说明模型学习能力不足，所代表的函数空间没法拟合数据空间。一般训练集上就能看到偏差，无法拟合训练集。

![微信截图_2](/树模型.assets/微信截图_2.png)

方差：是模型在用同一数据集的不同采样进行训练后，预测结果的稳定性。如果不同采样数据集下训练出来的模型，预测结果都差不多（方差较小），说明模型对输入样本的分布不敏感，能够较好的捕捉不同数据集中共性的规律，有较好的泛化能力。而复杂模型学的太细，容易过拟合，如果数据集稍有改变，拟合曲线就会发生较大变化，导致方差较大。如果线性模型，尽管单个模型偏差大，但对数据的扰动更加不敏感，方差小：

![微信截图_20210207201541](/树模型.assets/微信截图_20210207201541.png)

更直观的看，越复杂的模型方差越大，对输入值越敏感，越容易过拟合：

![微信截图_20210207204452](/树模型.assets/微信截图_20210207204452.png)

#### 集成学习

**Bagging**：综合多个不同的模型得到的结果进行预测。这样得到的结果往往好于独立的单个模型。因为独立实验次数越多，得到的结果越接近于数据的真实值（大数定理）。即使是稍微好于随机预测的弱模型（准确率51%），单个模型犯错的概率是49%，如果有多个这样的独立模型，综合犯错的概率会很低（集体智慧/3个臭皮匠）。因此可以通过对弱模型的组合，来得到强模型。

严格来说，如果每个弱分类器预测错误的概率是：
$$
P(f(x)!=y)=\epsilon
$$
N个弱分类器的预测结果是所有模型结果的综合（bagging是投票，boost是加权求和）：
$$
F(x)=sign(\sum_{i=1}^{N}f(x))
$$
只有超过一半的分类器预测错误，最终的模型才会预测错误。因此集成后，模型最终预测错误的概率为其中至少一半基模型预测错误的概率。根据Hoeffding不等式，模型的预测误差会随着基模型数目的增多而指数减小。理论上讲，无数个独立模型可以让最终的预测误差趋近于0：
$$
P(F(x)!=y)=\sum_{k=0}^{N/2}C_N^k \epsilon^k(1-\epsilon)^k \le e^{(-\frac{1}{2}N(2\epsilon-1)^2)}
$$

因此在项目的最后阶段，往往用不同的模型分别做预测，综合各个模型的结果，得到更好的效果（ensemble）。

另一方面，从方差偏差的角度讲，综合多个基模型的结果进行预测，整体的方差会降低为原来的$1/n$，最终得到的预测结就有小的方差，预测结果有较好的稳定性。

为了增加不同模型的独立性，一般选择不同的采样数据集/不同模型/不同特征进行训练。最好选择方差较大，对样本分布较敏感的模型作为基模型(如决策树，神经网络)，使得不同的采样下，模型会有较大不同。Bagging采用有放回抽样得到子数据集：如果原始训练集数目为$m$, Bagging会有放回的方式抽$n$次，得到大小为$n$的采样集($n$可以等于$m$)。其中有的样本被采样多次，有的样本没有被采到。这个过程重复T次，得到T个采样集，用来训练每个基模型。这些采样集中的模型分布不同，因此训练得到的基模型也相对独立。而对每个采样集而言，原始数据集中约有36.8%的样本未使用，因此可以用这些包外样本对该基模型进行评估，降低模型整体的泛化误差，不需要额外的验证集。

但bagging模型不会降低基模型的偏差，模型整体的偏差基本近似于基模型的偏差，所以一般采用偏差小的强模型做基模型。对于决策树，可以不剪枝，增大树的深度。但同时，bagging也应尽量避免选择对样本分布不敏感，方差较小的稳定模型做基模型（SVM，线性回归等）：基模型本身方差就小，不同采样得到的基分类器基本相同。集成就不能改善模型。本质上，集成是通过对基模型预测结果的平滑（消除不同意见，留下共识），让模型的结果变好。当基模型很稳定时，集成的效果有限。但如果基模型不稳定但准确时，集成能有效降低误差。

bagging有放回采样会使得模型独立性增加，样本分布有较大的不同。也可以考虑patching（在采样每个子数据集时无放回），但会使基模型独立性较低。

**Boost**：用不同模型逐渐对预测值进行逼近，每一步都聚焦于之前分错的样本，逐步减小偏差。由于各模型之间并不独立，boost不能降低方差。boost方法的方差近似于基模型的方差。因此可以选择方差小的弱模型（弱模型,如lr）作为基模型。

#### 为什么决策树对输入样本敏感？

决策树的训练结果非常依赖于训练样本的分布情况。训练集中样本分布的微小改变，会导致子数据集的条件熵$H(D|A)$/基尼指数$G_ini(D,A)$的值发生改变，使得最终选择的特征不同，树结构改变。因此决策树对样本的整体分布较为敏感。尤其在较深的情况下，有较高的方差，较低的偏差。

其他分类器如果对样本分布不敏感，不同数据下bagging得到的模型很相似，非独立模型，很难通过bagging的方式降低泛化误差。和一个模型没有太大差别，方差不会明显降低。

#### 随机森林

为了进一步增加不同基模型之间的独立性，相比于一般的Bagging方法，随机森林对样本特征也进行采样：对随机森林中的每个固定基模型，每次每个节点都从所有特征中随机选择k个特征，从中选择最优特征作为该节点的切分点。最后的结果是Vote，硬投票（类别）或者软投票（概率）。

为了让每个基模型不同，也不对基模型进行剪枝。每个基模型都有较小偏差，较大方差。

#### 特征重要性

一般是树选择该特征切分时，整体降低的基尼指数（纯度）。降低的纯度越多，说明该特征越重要。但这样得到的特征重要性会更偏向于取值多的特征（高基特征，如ID）。可以选择Permutation feature importance，对每个特征进行打乱(样本间)，看验证集上模型的表现下降的程度：该特征打乱后性能下降越多，该特征越重要。可以有效避免上述bias。

但是一般Permutation打乱后需要重新训练，再在测试集上验证。为了不进行重新训练，就得到特征的重要程度，随机森林可以就直接看特征打乱后，在out-of-bag样本上性能的下降程度。

#### ADaBoost

Adaboost主要用来解决二分类问题。作为一种boost方法，Adaboost在训练每个弱分类器时，会更加关注上一个分类器中预测错误的样本，增大这些样本在下一次训练时的权重，而降低那些已经预测正确的样本的权重，使得下一个分类器在训练时，更加关注上次预测错误的那些样本，从而不断减小偏差。最终的分类器是所有的弱分类器$G_n(x)$根据预测能力$\alpha_n$得到的加权组合$\{-1,1\}$：
$$
f(x)=sign(\sum_{i=1}^{n}\alpha_nG_n(x))
$$
具体来说，每个弱分类器都基于给定的样本权重$D=\{w_1,w_2,...,w_N\}$进行训练。$D$是一个概率分布：$\sum_{i}w_i==1$。初始时，所有样本权重相等：$\frac{1}{N}$

之后根据样本权重训练每个基模型。如果使用逻辑回归作为基模型，权重大的样本在loss中占更大比重。如果用SVM作为基模型，不同样本的惩罚因子不同，权重较大的样本，对不满足严格间隔的惩罚更大：

![image-20210223233357902](/树模型.assets/image-20210223233357902.png)

如果用决策树作为基模型，为了使用样本权重进行训练，不是通过基尼指数等统计指标来选择最佳切分点，而是用切分后所有分类错误样本对应的加权分类差误率作为指标，选该值最小的切分点进行切割：
$$
e=\sum_{i=1}^{N}w_iI(G_m(x)!=y_i)
$$
其中$w_i$是样本$i$在本次训练时的权重。权重越大，该样本越容易被分类正确。第$m$个模型训练结束后，得到最终的分类误差率$e_m$以及该模型的（对数）预测几率$\alpha_m$，可以反应该基分类器的预测效果，作为该模型最终的权重(如果稍好于随机猜测，$\alpha_m>0$。否则可以反着用，权重是负，样本权重反向调整)：
$$
\alpha_m=\frac{1}{2}ln(\frac{1-e_m}{e_m})
$$


之后调整样本权重分布，增大本次训练中预测错误样本的权重（变为原来的$e^{\alpha}$倍，大于1），减小预测正确样本的权重（变为原来的$e^{-\alpha}$倍，小于1），归一化得到新的权重分布$D_{m+1}=\{w_{m+1,1},w_{m+1,2},...,w_{m+1,N}\}$，用于训练下一个分类器：
$$
w_{m+1}=\{
	\begin{aligned}
	{w_me^{\alpha}}&,          G_m(x)!=y_i  \\ 
	{w_me^{-\alpha}}&,         G_m(x)==y_i 
\end{aligned}={w_me^{-\alpha G_m(x)y_i}}
$$
其中预测错误$G_m(x)y_i==-1$。之后对调整后的样本权重分布进行归一化，使其仍是概率分布。之后基于新的样本权重来训练下一个分类器。

最终得到的分类器是所有基分类器的加权求和，如式（16）。因为每个样本都去纠正了前一个模型的犯错样本，所有模型综合得到的结果，是很少再有犯错的样本。可以对整体数据集进行较好的拟合，降低偏差。

#### GBDT

###### 引子：梯度下降法

对于某个函数$f(w)$。其在参数$\vec{w_0}$处的一阶泰勒展开是：
$$
L(\vec{w})=L(\vec{w}_0)+(\vec{w}-\vec{w}_0)*g(\vec{w}_0)
$$
为了最小化损失函数$L(w)$，参数初始值确定后，需要让参数沿着函数值下降最快的方向(负梯度方向）进行更新，使更新量$\Delta w$与函数$f$在该点的梯度$g$反向：
$$
\Delta w=-g(w)
$$


为了尽快下降到最小值，应该让参数$w$每次都沿着负梯度的方向进行更新，使$f(w)$下降到局部最优：
$$
\vec{w}_{m+1}=\vec{w}_{m}-g(\vec{w}_m)
$$


其中$g({w}_m)$是函数$L(w)$在点${w}_m$处的梯度，是函数上升最快的方向。

###### GBDT核心思想:

相比于adaboost每次都关注上一次预测错误的样本，作为一种boost方法，gbdt通过逐步优化预测函数$F(x)$来减小损失函数。

GDBT将最终的预测函数$F(x)$看作是损失函数$L$的参数，通过梯度下降法来不断更新$F(x)$。如果一般的梯度下降法是让参数$w$在参数空间进行更新，那么GBDT则是让预测函数$F(x)$在函数空间内进行更新。类似于参数的更新，在当前预测函数$F_{m-1}(x)$基础上训练下一颗树时，更新量就是下一颗树要拟合的函数值：
$$
F_m(x)=F_{m-1}(x)+\alpha*f_m(x)
$$
为了通过每次更新来逐步减小损失函数$L(F)$，GBDT的每棵树都拟合损失函数此刻在$F(x)$上的的负梯度(不考虑步长情况下)，
$$
f_m(x)=-\frac{\partial L}{\partial F}|_{F=F_{m-1}(x)}
$$
从而使F(x)在可以在负梯度方向上不断更新（累积新的树），从而逐步降低损失，减小偏差（类比式21）:

$$
F_m(x)=F_{m-1}(x)+\alpha*-\frac{\partial L}{\partial F}|_{F=F_{m-1}(x)}
$$
 因此初始的基模型$F_0$确定后，剩下的每个基模型$f_m(x)$都需要根据不同的损失函数，拟合对应的负梯度。最终的模型是预测函数$F(x)$在负梯度方向上的累积（类似于一般梯度下降中的参数更新）。最终的预测结果基于此得到。
$$
F(x)=F_{0}(x)+\sum_{m}\alpha_mf_{m}(x)
$$
由于每棵树需要预测的都是负梯度(连续值)，因此每个基模型都采用CART回归树去拟合。

由于不同的任务对应不同的损失函数，因此GBDT根据损失函数，让模型预测不同的$F_(x)$和对应的负梯度。

###### 回归问题：

对回归问题而言，损失函数是数据集上的平方误差：$L=\frac{1}{2}\sum_\limits{i}^{N}(y_i-\hat{y}_i)^2$，因此可以用模型$F(x)$直接拟合预测值$\hat{y}_i$。损失函数可以表示为：
$$
L=\frac{1}{2}\sum_\limits{i}^{N}(y_i-F(x_i))^2
$$
因此在回归问题中，下一个基模型需要去拟合的负梯度，正好是模型对每个样本当前的预测值$\hat{y}_i=F_{m-1}(x)$与样本真实值$y_i$（连续值）之间的残差$r_i$：
$$
f_m(x)=-\frac{\partial L}{\partial F}|_{F=F_{m-1}(x)}=y_i-F(x_i)|_{F=F_{m-1}(x)}=y_i-\hat{y}_i|_{\hat{y}_i=F_{m-1}(x)}
$$
因此可以将模型初始值$F_0$置为0，作为模型$F(x)$梯度下降的起点。之后每次都用一颗树去拟合训练集上剩余的残差，得到新的回归树$f_m(x)$和新的预测结果$\hat{y}_i=F_{m}(x)$。之后可以基于此，重新计算训练集上剩余的残差$r_i$，作为样本label，去训练下一个基模型。最终的预测结果是每个基模型贡献的预测结果（残差）的累积：
$$
\hat{y}_i=F(x)=F_{0}(x_i)+\sum_{m}f_{m}(x_i)
$$
因此回归问题中，GBDT本质是通过不断减小模型的残差来降低偏差的。第一个基模型拟合的残差$r_i$正好是样本的真实值$y_i$（此时模型的预测值是$\hat{y}_i==0$）。作为一种Boost方法，一般选择简单模型降低方差，因此$f_1(x)$用一个只有两个叶子节点的简单树来拟合，找到能够最小化平方损失的某切分点。之后每训练一个新的基模型，都去拟合现在剩余的残差，使模型总的残差越来越小，降低模型的预测偏差。

为了在每一步后尽可能减小训练损失$L$，每个基模型通过拟合残差$r_i$得到树结构后，不直接将叶节点此时的值作为该基模型的预测结果，而是用线性搜索的方法，搜索每个叶节点$t$对残差的最佳拟合值$\hat{c}_{mt}$，作为基模型$f_m(x)$最终的预测结果:
$$
\hat{c}_{mt}= \mathop{\arg\min}_{c} \ \ \ \sum_{x_i\in{R_{m,t}}}L(y_i,F_{m-1}(x_i)+c)
$$
其中样本空间$R_{m,t}$是第$m$个基模型的叶节点$t$的代表的样本空间。落在每个叶节点的样本分别进行优化，得到该叶节点对残差的最佳拟合值。因此回归模型最终的预测结果是每个基模型的每个叶节点对该样本拟合值的和：
$$
\hat{y}_i=F(x)=\sum_{m}f_{m}(x_i)=\sum_{m}\sum_{t}c_{mt}I(x_i\in R_{m,t})
$$
而回归问题一般直接拟合残差就可以，不必进行线性搜索。因为回归问题的损失如下：优化每个叶节点的拟合值$c$，等同于最小化式31最右边的误差，正好对应于每个CART回归树训练时的拟合目标。在树结构固定的情况下，使$L$最小的叶节点预测值就是落在该节点的所有训练样本的残差均值。
$$
L(y_i,F_{m-1}(x_i)+c)=\frac{1}{2}\sum_\limits{i}^{N}(y_i-F_{m-1}(x_i)-c)^2=\frac{1}{2}\sum_\limits{i}^{N}(r_i-c)^2
$$

###### 分类问题的损失：

一般用KL散度来衡量两个不同分布之间的差异，如真实的类别分布$p$是[1,0,0],但预测出来的分布$q$是[0.7,0.2,0.1]，那么两个分布之间的差距可以用KL散度来衡量：
$$
KL(p||q)=\sum_{i}p_ilog(\frac{p_i}{q_i})
\\=\sum_{i}p_ilog({p_i})+\sum_{i}p_ilog(\frac{1}{q_i})
\\=-\sum_{i}p_ilog(\frac{1}{p_i})+\sum_{i}p_ilog(\frac{1}{q_i})
\\=-\sum_{i}p_ilog({p_i})+-\sum_{i}p_ilog({q_i})
\\=-H(x)+H(x)^{’}
$$
其中第一项$H(x)$是样本真实的信息量，是真实分布已知的情况下，编码每个类所需要的平均编码长度(期望)。而第二项$H(x)^{’}$是真实分布未知的情况下，用估计分布$q$对真实分布进行编码所需的平均编码长度。其中每个类别的编码长度采用估计分布：$log(\frac{1}{q_i})$，而样本本身仍服从真实分布。最终的KL散度反映的是用估计分布进行编码后相比于原来需额外增加的信息量：
$$
H(x)=1*log1=0 bits\\
H(x)^{’}=1*log\frac{1}{0.7} + 0 *log\frac{1}{0.2} + 0 *log\frac{1}{0.1} =0.154 bits
$$
为了让预测分布尽可能接近真实分布，需要尽可能减小预测分布与真实分布之间的KL散度。但一般情况下$H(x)$确定(真实分布)，因此分类问题选择交叉熵函数$H(x)^{’}$作为损失函数，使预测得到的类别分布$q:\{q_1,...q_k,...,q_K\}$尽可能接近样本的真实分布（one-hot）：
$$
L=-\sum_{i}p_ilog({q_i})=-\sum_{i=1}^Ky_ilog(q_i(x))
$$
其中$y_i$对应真实分布，$q(x)$为预测类别分布。真实分布一般为one-hot形式：$\{0,0,...1,...,0\}$。也可以进行软编码防止过拟合，如改为$\{0.05,0.05,...0.9,...,0.05\}$。

参考：[知乎：KL散度与交叉熵](https://www.zhihu.com/question/345907033/answer/823359858)

###### GBDT二分类：

同逻辑回归，在只有2类的情况下，GBDT单个样本的损失可以由式34简化为：
$$
L(y_i,F(x))=-\sum_{i}y_ilog(p_i(x))= -y_ilog(p(y=1))-(1-y_i)log(p(y=0))\\=-(y_ilog(p)+(1-y_i)log(1-p))
$$

其中$p$是模型将该样本预测为正的概率。如果从极大似然的角度看，我们希望样本真实label为1时，预测其为1的概率最大，真实label为0时，预测其为0的概率最大：
$$
P(\theta)=\{
	\begin{aligned}
	P(yi=1)&,          y_i=1  \\ 
	P(yi=0)&,         y_i=0 
\end{aligned}=\begin{aligned}
	p&,          y_i=1  \\ 
	1-p&,         y_i=0 
\end{aligned}=p^{y_i}(1-p)^{1-y_i}
$$
对应的对数似然概率是：
$$
logP(\theta)=y_ilog(p)+(1-y_i)log(1-p)
$$
极大化似然概率就是极小化$logP(\theta)$，同式35。

而GBDT二分类模型将样本预测为正例的概率是sigmoid函数（同逻辑回归）：
$$
p(y_i=1)=sigmoid(F(x))=\frac{1}{1+e^{-F(x)}}
$$
逻辑回归用线性模型$w^Tx$拟合其中的$F(x)$（也就是$z$）。而GBDT用总的模型来拟合其中的$F(x)$。在模型输出$F(x)$后，求sigmoid得到将样本预测为正例的概率。而稍作计算可知，sigmoid函数中的$F(x)$就是预测得到的对数几率(odds)。因此GBDT整个模型拟合的就是预测的对数几率$F(x)$。
$$
F(x)=log\frac{p}{1-p}
$$
将式37带入损失函数式36，可以得到损失函数$L$对于模型对单个样本的总体拟合值$F(x)$的表达式：
$$
L = -y_ilog(p)-(1-y_i)log(1-p)\\=-y_ilog(p)-log(1-p)+y_ilog(1-p)
\\=-y_ilog(p)+y_ilog(1-p)-log(1-p)
\\=-yilog(\frac{p}{1-p})-log(1-p)
\\=-y_iF(x)-log(1-p)
$$
其中预测为正的概率$p$是$F(x)$的sigmoid函数。那么损失函数$L$对模型当前预测函数$F_{m-1}(x_i)$的负梯度是：
$$
r_m(x)=-\frac{\partial L}{\partial F}|_{F=F_{m-1}(x)}\\=-(-y_i-\frac{-1}{1-p}*\frac{\partial p}{\partial F})
$$
而sigmoid函数$f(z)$对z的梯度$f(z)'=f(z)f(1-z)$，因此损失函数对模型输出值的负梯度可进一步表示为：
$$
r_m(x)=-\frac{\partial L}{\partial F}|_{F=F_{m-1}(x)}\\=-(-y_i-\frac{-1}{1-p}*p*(1-p))\\=-(-y_i+p)\\=y_i-p|_{F(x)=F_{m-1}(x_i))}
$$
因此每颗基模型要拟合的是此刻样本被预测为正例的概率$p$与样本真实label之间的残差。此时计算预测概率用到的函数，综合了该样本在之前所有基模型上的预测结果，每个基模型贡献一部分对数几率函数的拟合值:
$$
p(y_i=1)=\frac{1}{1+e^{-F_{m-1}(x_i)}}=\frac{1}{1+e^{-\sum_{i=1}^{m}{f_m(x_i)}}}
$$
由于最终要拟合的$F(x)$是对数几率，可以将模型拟合的起点设置为训练集真实分布上的对数几率，所有样本都拟合该值：
$$
F_0(x_i)=log\frac{P(yi==1)}{P(yi==0)}=log\frac{P}{1-P}
$$
其中$P$是训练集上样本为正例的比例。拟合后每个样本的模型被预测为正例的概率p接近训练集中正例的正式分布P。之后训练m个树，每个树将此刻所有样本的残差$r_m(x_i)$作为label，训练新的模型。同样采用CART回归树做拟合，得到第m个回归树$f_m(x)$，并对这颗树每个叶子节点的拟合值进行优化（参考式29，式30）。

模型最终的输出$F(x_i)$是每个子模型拟合的负梯度的累积。预测某样本是正例的概率是基于$F(x)$的Sigmoid函数：
$$
p(y_i=1)=sigmoid(F(x_i))=\frac{1}{1+e^{-F(x_i)}}=\frac{1}{1+e^{-\sum_{i=1}^{m}{f_m(x_i)}}}
$$
[GBDT二分类：最优化基模型拟合值](https://zhuanlan.zhihu.com/p/89549390)

[GBDT分类](https://blog.csdn.net/wuzhongqiang/article/details/108349729)

###### GBDT多分类：

多分类场景下，单样本损失函数是预测分布与真实分布间的交叉熵损失，同式34：
$$
L=-\sum_{k=1}^Ky_klog(p_k(x))
$$
而$p_k(x)$是样本被预测为第$k$类的概率，采用softmax函数对模型输出做归一化而得到的：
$$
p(Y=1|x)=\frac{e^{F_1(x)}}{\sum_\limits{i=1}^{K} e^{F_i(x)}}\\
p(Y=2|x)=\frac{e^{F_2(x)}}{\sum_\limits{i=1}^{K} e^{F_i(x)}}\\ \\
...\\ \\ 
p(Y=K|x)=\frac{e^{F_K(x)}}{\sum_\limits{i=1}^{K} e^{F_K(x)}}
$$
而$F_1(x),F_2(x),...,F_K(x)$是模型为每个类输出的预测函数。因此在多分类时，模型不是输出一个函数$F(x)$,而是为每个类输出一个$F_i(x)$，最后根据上式得到单样本被预测为每个类的概率。因此损失函数$L$是关于$K$个预测函数的损失：
$$
L=-\sum_{i=1}^Ky_klog(p_k(x))=-\sum_{i=1}^Ky_klog\frac{e^{F_i(x)}}{\sum_\limits{i=1}^{K} e^{F_i(x)}}
$$
因此训练时，每一轮都需要训练$K$个树，分别去拟合$L$对每一类预测函数$F_k(x)$的负梯度：
$$
r_{m,k}(x)=-\frac{\partial L}{\partial F}|_{F_{k}(x)=F_{m-1,k}(x)}
\\=y_k-\frac{e^{F_{m-1,k}(x)}}{\sum_\limits{i=1}^{K} e^{F_{m-1,i}(x)}}
\\=y_k-p(Y=k|x)
$$
其中$y_k$是样本的真实类别被编码为one-hot向量后，向量在第$k$维的取值。而$p(Y=k|x)$是此刻样本被预测为第k类的概率。因此每$m$轮的每个类对应的基模型$f_{m,k}$要拟合的目标$r_{m,k}(x)$是样本被预测为该类的概率，样本在该类上真实Label 间的残差。而计算$p(Y=k|x)$的预测函数$F_k(x)$只在该类对应的基函数上累加：
$$
F_{m,k}(x_i)=F_{m-1,k}(x_i)+f_{m,k}(x_i)=\sum_{m} f_{m,k}(x_i)
$$
在选定K个类对应的预测函数起始点$F_{0}=0$后，每一轮训练$K$个基模型，得到$K$个新的预测函数
$$
F_1(x),F_2(x),...F_k(x),...,F_K(x)
$$
基于新的预测函数,可以得到该样本的新的分布：
$$
{p(Y=1|x),p(Y=2|x),...,p(Y=k|x),...,p(Y=K|x)}
$$
计算该样本的预测类别与样本真实标签之间的残差，作为该样本在下一次要训练的新的K个基模型中的label。其中样本$x$在新一轮中第$k$个基模型中的label是$r_{k}(x)$：
$$
r_{1}(x),,...,r_{k}(x),...,r_{K}(x)=\{0,...,1,...0\}-\{p(Y=1|x),...,p(Y=k|x),...,p(Y=K|x)\}
$$
训练$m$轮，得到每个类最终的预测函数$F_1(x),F_2(x),...F_k(x),...,F_K(x)$。其中$F_k(x)=\sum_\limits{m} f_k(x)$。最终预测结果$p_k$取softmax,同式（46）。

假设数据集中样本有3类，某样本是第二类，被编码为[0，1，0]。那么以该样本为例，每一轮的训练过程如下：

![image-20210306221718381](/树模型.assets/image-20210306221718381.png)

其中每个基模型都只基于样本在对应类上的残差进行训练。而和GBDT二分类相同，GBDT多分类过程中每次得到树结构后，同样需要通过最优化，来得到每棵树的最佳残差拟合值。

[GBDT算法用于分类问题](https://zhuanlan.zhihu.com/p/46445201)

[深入理解GBDT多分类算法](https://zhuanlan.zhihu.com/p/91652813)

#### XGBoost







