# 								树模型

#### 决策树的训练与预测：

**训练**：

决策树训练时，会根据训练集选择每个特征(+取值)来划分数据集，直到数据集不可分或规模太小/树深度达到阈值/训练误差小于阈值（回归）/特征的信息增益小于阈值/数据集基尼指数小于阈值（够纯）。这时会停止分割，将落入该节点的多数训练样本的类别作为该节点的最终预测类别（分类）/  将落入该节点的所有训练样本的均值，作为该节点的最终预测值(回归。训练误差最小)。

训练结束后，会保存树的结构以及每个节点的信息，包括内部节点的特征(以及取值）和叶子节点的预测类别（分类/预测值(回归)。此外，训练过程中在每一步计算得到的特征重要性(分类：信息增益/信息增益比/基尼指数，回归：预测误差)也会保存下来

**预测**： 将样本特征与训练得到的特征（和取值）进行比较，递归向下，落到某叶节点，得到最终的预测分类/预测值

#### 特征选择与特征处理：

**ID3：**只能处理离散特征。每次都选择当前信息增益$g(D,A)$最大的特征$A$作为分割节点。该特征的信息增益是原始数据集上的信息熵(相对类别)与被特征$A$分割后的子数据集上的条件熵$H(D|A)$的差值:
$$
g(D,A)=H(D)-H(D|A)
$$
其中条件熵$H(D|A)$ 是特征$A$的每个取值$A_i=c$对应的子数据集$D_i$的加权熵($p_i$是特征$A$取值为$c$的样本占比)：
$$
H(D|A)=p_i\sum_{i=1}^KH(Di)
$$
信息增益本质上是因为知道了特征A而减少了的不确定性。该值越大，说明特征A带来的信息量越多。选择特征A能够最大的减少剩余信息的不确定性，使得子数据集整体的熵减小，划分后子数据集比较纯，分类较好。本质上是选择让剩余数据集信息量(熵)最小的特征。

由于ID3只能处理离散特征，所以每个节点对应的子数都是多分叉树，对应该特征的所有取值。因此在用完该特征后，下一次进行分割时，子数据集应丢弃用过的特征。

**C4.5:**   ID3倾向于首先选择取值比较多，但样本数少的特征（高基数特征，high-cardinality），前提是某个特征的每个取值下的样本数非常少（这时候条件熵$H(D|A)$会较小。否则没有明显的偏向性）。因为这样会迅速把样本空间划分的过小，增加过拟合的风险。比如ID类特征（IP，地址，门牌，姓名）。用这类特征划分子数据集后，每个子数据集只有少量样本，划分后每个子数据集熵都是0，信息增益大，但分割后难以泛化。为了避免首先选择这类特征，需要对这类特征进行惩罚。C4.5用信息增益比代替信息增益，用特征本身的熵做惩罚：



​           
$$
g_R(D.A)=\frac{g(D,A)}{H_A(D)}
$$
​        其中$H_A(D)$是关于特征A本身的熵：$-p(Ai=c)log(p(Ai=c))$。熵越大，说明特征本身不确定性较大，取值可能较多且等概率，会把样本空间划分的较小，不太好泛化 。以ID为例，有n种取值，每种取值等概率，熵就会比较大，用特征的熵做乘法，就可以尽量优先选那些取值较少的特征:
$$
H_A(D)=-\sum_{i=1}^Kp(A_i)log(p(A_i))=-\sum_{i=1}^n\frac{1}{n}log\frac{1}{n}=-log\frac{1}{n}=logn
$$
​       但是信息增益率本身引入了特征的分布属性，偏向于分布不平衡（特征熵较小）的特征。而信息增益本身更能够反应划分的好坏。因此为保证划分质量，首先会先计算所有特征的平均信息增益作为阈值。只有大于该信息增益的特征，才会进一步用信息增益率做选择。

​        对于离散特征，C4.5同ID3, 也是选择该特征所有的可能取值进行多子树划分，特征用完之后就丢弃掉。只是多加了特征熵的惩罚。

​         对连续性特征，C4.5会对将该样本的取值离散化，将该特征在有限训练样本中的所有取值进行排序，取所有中间值作为分割点进行二分分割。因为都是二分分割，在比较该特征的所有取值时，若选择信息增益率作为指标，在分割点差不多的情况下，会惩罚等分分割点（特征熵比较大）。在能够较好划分样本的情况下，可能因为惩罚而选择不平衡的分割点。因此在选择连续特征的最优分割点时，只用分割点处的信息增益做指标，尽量将两部分数据分开。

​       选定每个连续特征的最优分割点后，需计算信息增益率，和离散特征进行比较（相当于都加上对取值的惩罚）。而对于每个连续型特征，在每一步选择完最优分割点后，若还有其他分割点，之后的数据集仍可以用这些分割点进行划分。

​    [参考： C4.5 连续特征的处理](https://www.cnblogs.com/wf-ml/p/10685499.html)

​    [参考：C4.5 连续特征的处理2](https://www.cnblogs.com/zhangchaoyang/articles/2842490.html)

**CART:** 二叉分裂。在特征选择时，遍历每个特征的每个取值，在所有组合中，选择最优的组合$(A,a_i)$对数据集进行二分切割，每一步不仅保存特征本身，也保存该特征的取值，以对新样本进行预测。离散特征的每个取值都是一个切分点，根据特征$A$取值是否是$ai$对数据集二分。连续特征的二分切割同C4.5，用该特征的所有取值的中间值做切分，同样是所有特征的所有取值同时比较，选择当前的最优切分。

​          对于分类问题，CART采用基尼指数来衡量切分的好坏：
$$
G_{ini}(D)=\sum_{i=1}^Kp_i(1-p_i)
$$
 其中$p_i$为该类的占比。类似于熵，基尼指数越小，说明数据纯度越高，切分得越好。因此选择切分后，加权基尼指数最小的特征$A$和对应取值$ai$，作为最优分割点：
$$
G_{ini}(D,A=a_i)=\frac{D1}{D}G_{ini}(D1)+\frac{D2}{D}G_{ini}(D2)
$$
而回归问题采用切分后，能最小化均方预测均方误差的切分点，作为最优当前特征取值组合。

#### 方差偏差：



#### bagging/boost:



#### 为什么决策树对输入样本敏感？决策树属于哪种模型？（高方差，低偏差）？

决策树的特征选择基于训练样本的统计量，特别是特征和类别分布情况。即使是同一特征，子数据集样本分布不同，条件熵$H(D|A)$/基尼指数$G_ini(D,A)$的值也会有较大差异，使得决策树的结构存在较大差异。不同于SVM等方法，仅依赖于支持向量，决策树对样本的整体分布较为敏感。训练集中样本分布的微小改变，也会导致决策树选择的特征不同，树结构发生较大改变。

#### bagging应该选择哪种模型（低方差，高偏差），boost应该选择哪种模型。对应决策树怎么搞



#### 决策树剪枝：（ID3不存在剪枝算法/C4.5存在/CAR预剪枝后剪枝）



#### 缺失值处理：





