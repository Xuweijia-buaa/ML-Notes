# 								树模型

#### 决策树的训练与预测：

**训练**：

决策树训练时，会根据训练集选择每个特征(+取值)来划分数据集，直到数据集不可分或规模太小/树深度达到阈值/训练误差小于阈值（回归）/特征的信息增益小于阈值/数据集基尼指数小于阈值（够纯）。这时会停止分割，将落入该节点的多数训练样本的类别作为该节点的最终预测类别（分类）/  将落入该节点的所有训练样本的均值，作为该节点的最终预测值(回归。训练误差最小)。

训练结束后，会保存树的结构以及每个节点的信息，包括内部节点的特征(以及取值）和叶子节点的预测类别（分类/预测值(回归)。此外，训练过程中在每一步计算得到的特征重要性(分类：信息增益/信息增益比/基尼指数，回归：预测误差)也会保存下来

**预测**： 将样本特征与训练得到的特征（和取值）进行比较，递归向下，落到某叶节点，得到最终的预测分类/预测值

#### 特征选择：

**ID3：**只能处理离散特征。每次都选择当前信息增益$g(D,A)$最大的特征$A$作为分割节点。该特征的信息增益是原始数据集上的信息熵(相对类别)与被特征$A$分割后的子数据集上的条件熵$H(D|A)$的差值:
$$
g(D,A)=H(D)-H(D|A)
$$
其中条件熵$H(D|A)$ 是特征$A$的每个取值$A_i=c$对应的子数据集$D_i$的加权熵($p_i$是特征$A$取值为$c$的样本占比)：
$$
H(D|A)=p_i\begin{equation*}

\sum_{i=1}^KH(Di)

\end{equation*}
$$
信息增益本质上是因为知道了特征A而减少了的不确定性。该值越大，说明特征A带来的信息量越多。选择特征A能够最大的减少剩余信息的不确定性，使得子数据集整体的熵减小，划分后子数据集比较纯，分类较好。本质上是选择让剩余数据集信息量(熵)最小的特征。

由于ID3只能处理离散特征，所以每个节点对应的子数都是多分叉树，对应该特征的所有取值。因此在用完该特征后，下一次进行分割时，子数据集应丢弃用过的特征。
