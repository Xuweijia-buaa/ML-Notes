## Transformer

官方代码：https://github.com/tensorflow/tensor2tensor

官方代码解读博客：http://fancyerii.github.io/2021/02/01/t2t-code-reading-4/

一个tensorflow版本，比较清晰：https://github.com/Kyubyong/transformer，对应的解析：https://www.cnblogs.com/xiximayou/p/13336729.html

Harvard_NLP(pytorch)也比较清晰：https://github.com/harvardnlp/annotated-transformer，对应的解析：https://xieyangyi.blog.csdn.net/article/details/105980363

一个带有beam-search做预测的版本（官方也有）：https://zhuanlan.zhihu.com/p/109183727?utm_source=wechat_session&utm_medium=social&utm_oi=53635367043072，对应的代码：https://github.com/jadore801120/attention-is-all-you-need-pytorch

## Bert

官方代码和预训练模型（tensorflow）：https://github.com/google-research/bert

hugging_face版本(tf2+pytorch)：https://github.com/huggingface/transformers

详解博客（基于hagging-face-pytorch）：https://xieyangyi.blog.csdn.net/article/details/106908787

## Bert之后的模型

Roberta

Roberta+Chinese WWM

ALBert(更轻性能更强)